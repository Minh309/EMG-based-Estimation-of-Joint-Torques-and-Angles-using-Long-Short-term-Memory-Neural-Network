{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4elnnebYFymJ"
   },
   "source": [
    "<center><h1>EMG-based Estimations of Joints Angles and Torquse Using Long Short-Term Memory Network</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUmXlf9RFyTC"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "x6ncuNJLwbop",
    "outputId": "46831a56-9f47-4230-8a8f-615e2158683f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.figure(dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYSY1R3zMlcq"
   },
   "source": [
    "## define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH9NNXjGMkwt"
   },
   "outputs": [],
   "source": [
    "features_num=112 #Number of features in the model\n",
    "out_num=8 # Number of outputs\n",
    "time_steps=50 # time step for training LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjRkKRTaEQEH"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p084FkpsMC09"
   },
   "source": [
    "### getting labels function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xpYPhzwyTZf"
   },
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    \"\"\"\n",
    "    Used to get Output labels as a numpy array. Labels are retrived from Subject 1 dataset\n",
    "    Labels are ['knee_angle_r', 'ankle_angle_r', 'knee_angle_l', 'ankle_angle_l',\n",
    "            'knee_angle_r_moment', 'knee_angle_l_moment',\n",
    "            'ankle_angle_r_moment', 'ankle_angle_l_moment']\n",
    "    Returns\n",
    "    -------\n",
    "    list contains all output labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    output_label =[\"Right knee angle\",  \"Right ankle angle\",\"Left knee angle\", \"Left ankle angle\",\n",
    "                \"Right knee torque\", \"Left knee torque\", \"Right ankle torque\", \"Left ankle torque\"]\n",
    "    return output_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI-LDwDtR1ck"
   },
   "source": [
    "### dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1uTp4ueQP15"
   },
   "outputs": [],
   "source": [
    "def get_dataset(subject_no):\n",
    "    \"\"\"\n",
    "    This function is used to retrive the dataset for selected subject. Dataset\n",
    "    will and last 8 columns contains the output. All columns are labeled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_no : string\n",
    "    Subject number in format XX.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas dataset\n",
    "    Subject number \"subject_no\" dataset without time column\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv('Dataset/Subject' + subject_no + '_dataset_RMS_MAV_50ms.csv',header=0)\n",
    "    return dataset.drop(columns=\"Time\").dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjNNNSeBQYdN"
   },
   "outputs": [],
   "source": [
    "def get_scaler(ndarray, features_range=(-1,1)):\n",
    "    \"\"\"\n",
    "    This function is used for creating scalers for scalling the features using\n",
    "    sklearn.preprocessing.MinMaxScaler function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ndarray : numpy arrayor pandas dataset\n",
    "    DESCRIPTION.\n",
    "    scale_range : Tuple, optional\n",
    "    Scaling range (Min,Max). The default is (-1,1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scaler : MinMaxscaler\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=features_range)\n",
    "    scaler.fit(ndarray)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXe_fcDnQhNh"
   },
   "outputs": [],
   "source": [
    "def scale_data(dataset, features_range=(-1,1), outputs_range=None):\n",
    "    \"\"\"\n",
    "    Scale input/output using MinMaxScaler. inputs are scaled in range (-1,1)\n",
    "    and outputs scaled in general model case only using range of (0,1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : numpy array or pandas\n",
    "      Dataset to be scaled.\n",
    "    features_range : tuple\n",
    "      The scaler range for input features. Default (-1,-1)\n",
    "    outputs_range : tuple or None\n",
    "      Scale all outputs in range the desired range. If None, no scaling will\n",
    "      be performed. Default value is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scaled_dataset : numpy array\n",
    "      a numpy array contains the scaled dataset.\n",
    "    output_scaler : MinMaxscaler\n",
    "      Output values scaler. Optional if Outpus_range is not None\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_values = dataset.values.astype(\"float32\")\n",
    "    input_scaler = get_scaler(dataset_values[:,:-out_num]) #get scaler for input features\n",
    "    input_dataset = input_scaler.transform(dataset_values[:,:-out_num]) #scale input features\n",
    "    if outputs_range==None:\n",
    "        output_dataset = dataset_values[:,-out_num:]\n",
    "        scaled_dataset = np.concatenate((dataset_values[:,:-out_num],output_dataset), axis=1)\n",
    "        return scaled_dataset\n",
    "    else:\n",
    "        output_scaler = get_scaler(dataset_values[:,-out_num:], scale_range=outputs_range)\n",
    "        output_dataset = output_scaler.transform(dataset_values[:,-out_num:])\n",
    "        scaled_dataset = np.concatenate((dataset_values[:,:-out_num],output_dataset), axis=1)\n",
    "        return scaled_dataset, output_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93Nz8PMGwcXF"
   },
   "outputs": [],
   "source": [
    "def create_inout_sequences(data_set):\n",
    "    \"\"\"\n",
    "    Create the input and output pairs sequances for the model\n",
    "\n",
    "      Parameters\n",
    "    ----------\n",
    "    input_data : numpy array\n",
    "        Subject dataset pair (inputs & Ooutputs).\n",
    "    features : int\n",
    "        Number of features in our model. default is 112\n",
    "    out_num : int\n",
    "        Number of outputs for the model to predict. outputs are arranged same way the get_label() function output.\n",
    "        default value is 8\n",
    "    time_steps : int\n",
    "        Time step for training LSTM model. default value is 50\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : numpy array\n",
    "        Numpy array with the shape (len(data_set)-time_step ,time_steps, features) contains the features.\n",
    "    outputs : numpy array\n",
    "        Numpy array with the shape (len(data_set)-time_step ,time_steps, out_num) contains all ouputs corrsponding to the features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    outputs =  []\n",
    "    L = len(data_set)\n",
    "    for i in range(L-time_steps):\n",
    "        inputs = data_set[i:i+time_steps,:features_num]\n",
    "        labels = data_set[i:i+time_steps,-out_num:]\n",
    "        features.append(inputs)\n",
    "        outputs.append(labels)\n",
    "    features = np.array(features)\n",
    "    outputs = np.array(outputs)\n",
    "    return features,outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbmiN0d1GjVd"
   },
   "source": [
    "## Create a function that will process all the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX8LYlXny_MG"
   },
   "outputs": [],
   "source": [
    "def in_out_process(subject, mod=\"training_set\", test_val_size=0.4, features_range=(-1,1), outputs_range=None):\n",
    "    \"\"\"\n",
    "    This function will process the dataset and give us the training, validation and test sets without shuffeling \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject: an \"XX\" string format.\n",
    "    mod: a string that tells the function about the goal of the recieved data set, is it for training or testing. Either \"training_set\" or \"test_set\".\n",
    "    test_val_size: float between [0,1), show the percentage of the validation and test sets from the total data set.\n",
    "      data will be as follow:\n",
    "      from 0 to (1-test_val_size)% for training\n",
    "      from (1-test_val_size)% to (1-test_val_size/2) for validation\n",
    "      from (1-test_val_size/2)% till the end of the data set for testing\n",
    "    features_range : tuple\n",
    "      The scaler range for input features. Default (-1,-1)\n",
    "    outputs_range : tuple or None\n",
    "      Scale all outputs in range the desired range. If None, no scaling will\n",
    "      be performed. Default value is None.\n",
    "\n",
    "    Returns:\n",
    "    X_train, X_val, y_train, y_val arrays as float32 tensors if mod is set for \"training_set\"\n",
    "    X_test, y_test if mod is set for \"test_set\"\n",
    "    \"\"\"\n",
    "    # in case of training\n",
    "    if mod==\"training_set\":\n",
    "        # Load the dataset\n",
    "        dataset = get_dataset(subject)\n",
    "        # Scale the dataset\n",
    "        scaled_dataset = scale_data(dataset, features_range=features_range\n",
    "                                   , outputs_range=None)\n",
    "        # get features and outputs\n",
    "        features, outputs = create_inout_sequences(scaled_dataset)\n",
    "        # get training set\n",
    "        X_train, X_test_val, y_train, y_test_val = train_test_split(features, outputs,\n",
    "                                                          test_size = test_val_size,\n",
    "                                                          random_state=None, shuffle=False)\n",
    "        # get validation\n",
    "        X_val, _, y_val, _ = train_test_split(features, outputs,\n",
    "                                              test_size = test_val_size/2,\n",
    "                                              random_state=None, shuffle=False)\n",
    "\n",
    "        print(\"Dataset_size: %2d, train_size: %2d, test_size: %2d\" %(len(features),\n",
    "                                                                    len(X_train),\n",
    "                                                                    len(X_val)))\n",
    "\n",
    "        return tf.constant(X_train,tf.float32), tf.constant(X_val,tf.float32), tf.constant(y_train,tf.float32), tf.constant(y_val,tf.float32)\n",
    "    # in case of testing\n",
    "    elif mod ==\"test_set\":\n",
    "        # Load the dataset\n",
    "        dataset = get_dataset(subject)\n",
    "        # Scale the dataset\n",
    "        scaled_dataset = scale_data(dataset)\n",
    "        # get test set starting point\n",
    "        test_len = int((1-test_val_size/2)*len(scaled_dataset))\n",
    "        # get test set\n",
    "        X_test = scaled_dataset[test_len: ,:-out_num]\n",
    "        y_test = scaled_dataset[test_len:,-out_num:]\n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DVSvg1kHv2V"
   },
   "source": [
    "## Model creation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01u3329ASu1Q"
   },
   "source": [
    "### define a function to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cYR4hoA3Y_X"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    keras.backend.clear_session() # Make sure we do not have any model in the notebook\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(4, return_sequences=True, input_shape=(time_steps, features_num),\n",
    "                              dropout=0.3 ,name=\"input_layer\"))\n",
    "    model.add(keras.layers.LSTM(4, return_sequences=True,\n",
    "                              dropout=0.3, name=\"2nd_layer\"))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(out_num), name=\"output_layer\"))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHFt2RYOSfSw"
   },
   "source": [
    "### Model Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQu9qE1L-0IW"
   },
   "outputs": [],
   "source": [
    "def col_arranger(df):\n",
    "    \"\"\"\n",
    "    Arrange columns to match the paper\n",
    "    \"\"\"\n",
    "    DF = df[['Left knee angle', 'Right knee angle', 'Left ankle angle', 'Right ankle angle',\n",
    "            'Left knee torque', 'Right knee torque', 'Left ankle torque', 'Right ankle torque']]\n",
    "    return DF\n",
    "\n",
    "def rmse_cal(Y_pred,Y):\n",
    "    \"\"\"\n",
    "    This function will calculate RMSE values\n",
    "    \"\"\"\n",
    "    size = Y_pred.shape[0]\n",
    "    rmse_value = np.sqrt(np.sum((Y_pred - Y)**2)/size)\n",
    "    return rmse_value\n",
    "\n",
    "def R2_cal(Y_pred,Y):\n",
    "    \"\"\"\n",
    "    This function will calculate R2 scores\n",
    "    \"\"\"\n",
    "    size = Y_pred.shape[0]\n",
    "    R2_value = 1 - np.sum((Y_pred - Y)**2)/np.sum(Y**2)\n",
    "    return R2_value\n",
    "        \n",
    "#Get the predicted values from the test dataset\n",
    "def get_prediction(inputs, model):\n",
    "    \"\"\"\n",
    "    get the predictions from inputs\n",
    "    inputs came in shape (length of the test set, features_num)\n",
    "    \"\"\"\n",
    "    m = len(inputs)\n",
    "    for i in range(0,m-50,50):\n",
    "        X = inputs[i:i+50,:]\n",
    "        X = np.reshape(X,(1,50,112))\n",
    "        if i == 0:\n",
    "            Y_preds = model.predict(X) \n",
    "        else:\n",
    "            y_pred = model.predict(X)\n",
    "            Y_preds = np.concatenate((Y_preds, y_pred))\n",
    "    return np.reshape(Y_preds,(-1,out_num))\n",
    "\n",
    "def evaluation(Y, Y_pred, Folder, labels=get_labels()):   \n",
    "    \"\"\"\n",
    "    Evaluate the model and plot the results\n",
    "    \"\"\" \n",
    "    time = [i*0.05 for i in range(len(Y_pred))]\n",
    "    frame_size=len(time)\n",
    "    r2_score = []\n",
    "    RMSE_score = []\n",
    "    for col in range(out_num):\n",
    "        rmse = np.round(rmse_cal(Y[:frame_size, col], Y_pred[:frame_size, col]),2)\n",
    "        R2 = np.round(R2_cal(Y[:frame_size, col],Y_pred[:frame_size, col]),4)\n",
    "        RMSE_score.append(rmse)\n",
    "        r2_score.append(R2)\n",
    "        if col == 0 or col == 1 or col == 2 or col == 3:\n",
    "            y_label = 'Degree'\n",
    "            unit = '\\xB0'\n",
    "        else:\n",
    "            y_label = 'Nm'\n",
    "            unit = ' Nm'\n",
    "        print('R2_score' + f'({labels[col]}) =', R2)\n",
    "        print('RMSE' + f'({labels[col]}) = {rmse}' + unit)\n",
    "        plt.plot(time[:frame_size], Y[:frame_size, col], 'r')\n",
    "        plt.plot(time[:frame_size], Y_pred[:frame_size, col], 'b--')\n",
    "        plt.xlabel('Time [s]', fontsize=14)\n",
    "        plt.xlim((0,50))\n",
    "        plt.ylabel(y_label, fontsize=14)\n",
    "        plt.xticks(fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.title(labels[col], fontsize=\"xx-large\")\n",
    "        plt.savefig(Folder + '/' + f'{labels[col]}.pdf')\n",
    "        plt.show()\n",
    "\n",
    "    return r2_score, RMSE_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSU9w6R43cs-"
   },
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTgGLTcGDA2u"
   },
   "outputs": [],
   "source": [
    "def train_eval(subject=None, lr=0.001, test_val_size=0.4,\n",
    "               load_best=False, eval_only=False, features_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    This function will train and evaluated the model\n",
    "    load_best: to load trainable saved model\n",
    "    eval_only: to skip training process and evaluate the model only, if it set to True, load_best will be True\n",
    "    \"\"\"\n",
    " \n",
    "    global Folder, model_path # Create model folder and path and make them global\n",
    "    Folder = \"S\" + subject\n",
    "    if not os.path.exists(Folder): #if folder doesn't exist, create one\n",
    "        os.makedirs(Folder)\n",
    "    model_path = Folder + \"/S\" + subject + '_model.hdf5' # Model path and name\n",
    "\n",
    "    output_label = get_labels() #import the labels\n",
    "    if not eval_only: #load dataset for training the model\n",
    "        X_train, X_val, y_train, y_val = in_out_process(subject, mod=\"training_set\",\n",
    "                                                        test_val_size=test_val_size, \n",
    "                                                        features_range=features_range)\n",
    "\n",
    "    model = create_model() # Create the model\n",
    "    try:\n",
    "        if load_best or eval_only: #load best model if we will evaluate the model only our we want to start training from pre-trained model\n",
    "            if os.path.exists(model_path): # make sure model exist\n",
    "                model.load_weights(model_path)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "          # Train the model\n",
    "        if not eval_only:\n",
    "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"MSE\") # Create model Compiler\n",
    "            # Define Check points for saving best model while training\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( \n",
    "            filepath = model_path,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True)\n",
    "\n",
    "            training = model.fit(x=X_train, y=y_train, batch_size=64, epochs=2000,\n",
    "                              validation_data=(X_val,y_val), \n",
    "                              callbacks=[model_checkpoint_callback])\n",
    "      \n",
    "            # Plot learning curve after the end of training\n",
    "            print(\"\\ntrain end\\n\\n Plotting Learning Curve\")\n",
    "            plt.plot(training.history[\"loss\"][100:])\n",
    "            plt.plot(training.history[\"val_loss\"][100:])\n",
    "            plt.title(\"Model loss\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "            plt.show()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTrain manually stopped\\n\\n\")\n",
    "\n",
    "    #Evaluate the model\n",
    "    # Get test set\n",
    "    X_test, y_test = in_out_process(subject, mod=\"test_set\", test_val_size=test_val_size, features_range=(-1,1))\n",
    "    # Get predictions\n",
    "    y_test_preds = get_prediction(X_test, model)\n",
    "    # Get R2 and RMSE results and plot measurements against predictions\n",
    "    r2_score, RMSE_score = evaluation(y_test, y_test_preds, Folder=Folder, labels=output_label)\n",
    "\n",
    "    # Return results as a pandas dataframe\n",
    "    r2_score = pd.DataFrame(data=np.array(r2_score,ndmin=2), columns=output_label)\n",
    "    r2_score.index = {\"S\" + subject}\n",
    "\n",
    "    RMSE_score = pd.DataFrame(data=np.array(RMSE_score, ndmin=2), columns=output_label)\n",
    "    RMSE_score.index = {\"S\" + subject}\n",
    "    # arrange columns and get the output\n",
    "    return col_arranger(r2_score), col_arranger(RMSE_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKMg_ww4U37D"
   },
   "source": [
    "## Train model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b5bzi2n6Rkd"
   },
   "outputs": [],
   "source": [
    "S1_r2, S1_rmse = train_eval(subject=\"01\", load_best=False, lr=0.001, eval_only=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF_Individual_LSTM_github.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
